{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출처;\n",
    "\n",
    "https://medium.com/onfido-tech/higher-level-apis-in-tensorflow-67bfb602e6c0 <br>\n",
    "http://bcho.tistory.com/1195"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data as mnist_data\n",
    "from tensorflow.contrib import slim\n",
    "from tensorflow.contrib.learn import ModeKeys\n",
    "from tensorflow.contrib.learn import learn_runner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimator\n",
    "\n",
    "Class이며, 모델, 학습 평가 될 사항들은 정함 input, Labels를 주면 train/eval/predict/export를 진행\n",
    "\n",
    "Pass사항은 3가지가 필요\n",
    "\n",
    "- parameter: 모델의 파라미터의 집합이어야 하며, dictionary가 될 수 도 있지만, HParams object (namedtuple)의 예로 보여주려함\n",
    "- configuration: 학습이 어떻게 평가되고 실행되는지 체크 후, 결과를 저장까지 시킴. RunConfig object에 진행됨. (Estimator가 학습에 필요한 모든 환경들을 실행한다.)\n",
    "- model function: python function이며, input을 기반으로 모델을 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "return tf.estimator.Estimator(\n",
    "        model_fn = model_fn, #first class function\n",
    "        params = params, #HPramas\n",
    "        config = run_config # Run Config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimator 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#데이터생성\n",
    "num_points = 300\n",
    "vectors_set = []\n",
    "for i in range(num_points):\n",
    "    x = np.random.normal(5,5)+15\n",
    "    y =  x*2+ (np.random.normal(0,3))*2\n",
    "    vectors_set.append([x,y])\n",
    "\n",
    "x_data = [v[0] for v in vectors_set]\n",
    "y_data = [v[1] for v in vectors_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/matplotlib/axes/_axes.py:545: UserWarning: No labelled objects found. Use label='...' kwarg on individual plots.\n",
      "  warnings.warn(\"No labelled objects found. \"\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1234ecf98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#시각화\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(x_data,y_data,'ro')\n",
    "plt.ylim([0,100])\n",
    "plt.xlim([5,35])\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model function\n",
    "\n",
    "모델 함수는 Estimator의 first class function이다. 이에 대한 장점은 함수가 instantiating 될 때마다 반응 할 수 있다는 것.<br>\n",
    "이게 무슨 뜻이냐 하면, 모델이 학습 중에 다양한 input을 받을 수 있다는 의미다. 예를 들면, training중에 validation testing을 동시에 할 수 있다는 것이다.<br>\n",
    "모델 funtion이 입력 받는것은 아래와 같으며, 모든 것은 __EstimatorSpec__ object로 return을 받아 모델을 완성시킵니다. <br>\n",
    "EstimatorSpec은 prediction, loss, training, evaluation operation들을 받아 full model graph에서 training, evaluation, inference를 진행힙니다. 또, EstimatorSpec은 일반적인 Tensorflow Operations를 받기 대문에, tf-slim과 같은 모델로 define 할 수 있습니다. <br>\n",
    "- input features: parameters\n",
    "- labels: tensor형태\n",
    "- mode: model이 학습인지 평간지, 추론을 하는 것인지 확인 하기 위한 것\n",
    "- hyperparameters: 파라메터를 받아야함\n",
    "\n",
    "\n",
    "\n",
    "* first class function\n",
    " - 1급 시민의 조건인, 변수를 담을수 있고, 인자 전달이 가능하며, 반환값을 전달하는 것에, 런타임 생성과 익명 생성이 추가 된 것\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimator 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#numpy 배열에서 데이터를 읽어서 리턴하기\n",
    "#x,y학습 데이터200까지 사용하고, 학습 epoch와 배치 사이즈를 정의 + 랜덤 셔플처리\n",
    "input_fn_train = tf.estimator.inputs.numpy_input_fn(\n",
    "    x = {\"x\":np.array(x_data[:200], dtype=np.float32)},\n",
    "    y = np.array(y_data[:200], dtype=np.float32),\n",
    "    num_epochs = 10000,\n",
    "    batch_size = 50,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "input_fn_eval = tf.estimator.inputs.numpy_input_fn(\n",
    "    x = {\"x\":np.array(x_data[200:300], dtype=np.float32)},\n",
    "    y = np.array(y_data[200:300], dtype=np.float32),\n",
    "    num_epochs = 10000,\n",
    "    batch_size = 50,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "input_fn_predict = tf.estimator.inputs.numpy_input_fn(\n",
    "    x = {\"x\":np.array([15,20,25,30], dtype=np.float32)},\n",
    "    num_epochs = 1,\n",
    "    shuffle = False\n",
    ")\n",
    "\n",
    "# Modeling\n",
    "# 읽어온 데이터에서, 어떤 컬럼을 학습에 사용할 지, 컬럼의 데이터 입력 (연속형 및 분류형)을 정한다.\n",
    "# 컬럼 명 x를 학습 데이터로 사용하고 x는 연속형 변수로 지정\n",
    "\n",
    "column_x = tf.feature_column.numeric_column(\"x\", dtype=tf.float32)\n",
    "columns = [column_x]\n",
    "\n",
    "#col.에 사용할 피쳐 목록이다.\n",
    "\n",
    "estimator = tf.contrib.learn.LinearRegressor(feature_columns=columns, optimizer=\"Adam\")\n",
    "\n",
    "#학습 및 예측\n",
    "#학습은 .fit이란 method를 사용\n",
    "estimator.fit(input_fn = input_fn_train, steps=5000)\n",
    "estimator.evaluate(input_fn = input_fn_eval,steps=10)\n",
    "result = list(estimator.predict(input_fn = input_fn_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 조금 더 쉽게 변경 해 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/ty/43yv2fzx4l9df6zcch8zml6m0000gn/T/tmpo13upsvx\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/ty/43yv2fzx4l9df6zcch8zml6m0000gn/T/tmpo13upsvx', '_tf_random_seed': 1, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer dense_1 is incompatible with the layer: : expected min_ndim=2, found ndim=1. Full shape received: [300]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-7ceca1d4a071>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEstimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps)\u001b[0m\n\u001b[1;32m    239\u001b[0m       \u001b[0mhooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStopAtStepHook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks)\u001b[0m\n\u001b[1;32m    628\u001b[0m           input_fn, model_fn_lib.ModeKeys.TRAIN)\n\u001b[1;32m    629\u001b[0m       estimator_spec = self._call_model_fn(features, labels,\n\u001b[0;32m--> 630\u001b[0;31m                                            model_fn_lib.ModeKeys.TRAIN)\n\u001b[0m\u001b[1;32m    631\u001b[0m       \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLOSSES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m       \u001b[0mall_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, labels, mode)\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'config'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_fn_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m     \u001b[0mmodel_fn_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_fn_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_fn_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEstimatorSpec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-7ceca1d4a071>\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(features, labels, mode)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mlayer1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mlayer2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/layers/core.py\u001b[0m in \u001b[0;36mdense\u001b[0;34m(inputs, units, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, trainable, name, reuse)\u001b[0m\n\u001b[1;32m    213\u001b[0m                 \u001b[0m_scope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                 _reuse=reuse)\n\u001b[0;32m--> 215\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    501\u001b[0m       \u001b[0mOutput\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m     \"\"\"\n\u001b[0;32m--> 503\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_assert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m           \u001b[0;31m# Check input assumptions set before layer building, e.g. input rank.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m           input_list = [\n\u001b[1;32m    439\u001b[0m               \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'input'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36m_assert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    565\u001b[0m                            \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m                            \u001b[0;34m'. Full shape received: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m                            str(x.get_shape().as_list()))\n\u001b[0m\u001b[1;32m    568\u001b[0m       \u001b[0;31m# Check dtype.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer dense_1 is incompatible with the layer: : expected min_ndim=2, found ndim=1. Full shape received: [300]"
     ]
    }
   ],
   "source": [
    "def input_fn():\n",
    "    features = tf.constant(x_data, tf.float32)\n",
    "    label = tf.constant(y_data, tf.float32)\n",
    "    return features, label\n",
    "\n",
    "def model(features, labels, mode):\n",
    "    layer1 = tf.layers.dense(features, 10)\n",
    "    layer2 = tf.layers.dense(layer1, 10)\n",
    "    out = tf.layers.dense(layer2, 1)\n",
    "    \n",
    "    global_step = tf.train.get_global_step()\n",
    "    loss = tf.losses.sigmoid_cross_entropy(labels, out)\n",
    "    train_op = tf.train.AdamOptimizer(0.001).minimize(loss, global_step)\n",
    "    \n",
    "    return tf.estimator.EstimatorSpec(mode = mode, loss=loss, train_op=train_op)\n",
    "\n",
    "est = tf.estimator.Estimator(model)\n",
    "est.train(input_fn, steps = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "import csv \n",
    "import os \n",
    "from sklearn import utils \n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "\n",
    "def parser(serialized_example):\n",
    "    features = {'sequence': tf.FixedLenFeature([2429], tf.int64), \n",
    "            'species': tf.FixedLenFeature([1], tf.int64)}\n",
    "\n",
    "    parsed_feature = tf.parse_single_example(serialized_example, features)\n",
    "    sequence = tf.cast(parsed_feature['sequence'], tf.int32)\n",
    "    species = tf.cast(parsed_feature['species'], tf.int32)\n",
    "    return sequence, species\n",
    "\n",
    "\n",
    "def input_fn_train(): \n",
    "    \n",
    "    ap = re.compile('dataset') # 발췌할 문자열 기입\n",
    "    file_dir= './dataset' # 데이터 경로 \n",
    "    files = os.listdir('./dataset/') # 경로에 해당하는 전체 파일들 \n",
    "      \n",
    "    train_data_all = list(filter(ap.search, files)) # 전체 파일들 중 발췌대상 문자열과 일치하는 파일들을 list 형태로 (데이터셋 10개)\n",
    "    \n",
    "    print(\"train:\", train_data_all)\n",
    "    \n",
    "    # 발췌된 전체 train_dataset 의 리스트 중 하나씩 빼온 것이 train_data. 이것이 데이터경로와 join 되어 filename 형성 \n",
    "    filename = [os.path.join(file_dir, train_data) for train_data in train_data_all]\n",
    "    filename.sort() \n",
    "    filename = filename[:8]\n",
    "    print(\"filename1:\",filename)\n",
    "    dataset = tf.contrib.data.TFRecordDataset(filename).map(parser)\n",
    "    dataset = dataset.batch(1000)\n",
    "    itr = dataset.make_one_shot_iterator()\n",
    "\n",
    "    sequence, species  = itr.get_next()\n",
    "\n",
    "    train_data = tf.one_hot(indices=sequence, depth=4)# depth 가 가로 길이 (종류)\n",
    "    train_label = tf.one_hot(indices=species,depth=11)\n",
    "\n",
    "\n",
    "    return train_data, train_label\n",
    "\n",
    "\n",
    "def input_fn_test():\n",
    "    \n",
    "    p = re.compile('dataset') # 발췌할 문자열 기입\n",
    "    file_dir= './dataset' # 데이터 경로 \n",
    "    files = os.listdir('./dataset/') # 경로에 해당하는 전체 파일들 \n",
    "    test_data_all = list(filter(p.match, files)) # 전체 파일들 중 발췌대상 문자열과 일치하는 파일들을 list 형태로 (데이터셋 10개)\n",
    "\n",
    "    print(\"test:\", test_data_all)\n",
    "    # 발췌된 전체 train_dataset 의 리스트 중 하나씩 빼온 것이 train_data. 이것이 데이터경로와 join 되어 filename 형성 \n",
    "    filename = [os.path.join(file_dir, train_data) for train_data in test_data_all]\n",
    "    filename.sort()  \n",
    "    filename = filename[8:]\n",
    "    print(\"filename2:\",filename)\n",
    "    dataset = tf.contrib.data.TFRecordDataset(filename).map(parser)\n",
    "    dataset = dataset.batch(1000)\n",
    "    itr = dataset.make_one_shot_iterator()\n",
    "\n",
    "    sequence, species  = itr.get_next()\n",
    "\n",
    "    test_data = tf.one_hot(indices=sequence, depth=4)# depth 가 가로 길이 (종류)\n",
    "    test_label = tf.one_hot(indices=species,depth=11)\n",
    "\n",
    "    return test_data, test_label\n",
    "\n",
    "\n",
    "\n",
    "def model_fn(features, labels, mode): \n",
    "\n",
    "    \"\"\" model function for estimator \"\"\" \n",
    "    train_op = None \n",
    "    loss = None \n",
    "    val_metric_ops = None\n",
    "\n",
    "   # filters = # of kernels ,  kernel_size = window size \n",
    "    conv1 = tf.layers.conv1d(features, filters=16, kernel_size=24, padding=\"SAME\")\n",
    "    maxpool = tf.layers.max_pooling1d(conv1,2429,strides=1)\n",
    "    fc = tf.layers.dense(maxpool, 32)\n",
    "    out = tf.layers.dense(fc,11) \n",
    "\n",
    "    print(\"out:\",out)\n",
    "\n",
    "    # mode = prediction\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT: \n",
    "        return tf.estimator.EstimatorSpec( \n",
    "            mode=mode, \n",
    "            predictions={\n",
    "                \"prob\":tf.nn.softmax(out)})\n",
    "\n",
    "    # mode = training\n",
    "    else: \n",
    "        global_step = tf.train.get_global_step() \n",
    "        loss = tf.losses.softmax_cross_entropy(labels, out) \n",
    "        train_op = tf.train.GradientDescentOptimizer(1e-2).minimize(loss, global_step) \n",
    "        accuracy = tf.metrics.accuracy(labels, tf.round(tf.nn.softmax(out))) \n",
    "        eval_metric_ops = {\"acc\": accuracy} \n",
    "        \n",
    "\n",
    "        return tf.estimator.EstimatorSpec( \n",
    "            mode=mode, \n",
    "            train_op=train_op, \n",
    "            loss=loss, \n",
    "            eval_metric_ops=eval_metric_ops) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "est = tf.estimator.Estimator(model_fn) \n",
    "\n",
    "\n",
    "for epoch in range(1): \n",
    "    est.train(input_fn_train,steps=3) \n",
    "    est.evaluate(input_fn_test,steps=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiement\n",
    "\n",
    "Experiement는 train model이 Estimator와 integrates를 할 수 있는 지 제공합니다. <br>\n",
    "\n",
    "input 리스트\n",
    "- estimator: 위에 설명\n",
    "- Train and eval. data as first class func: operation대신 function을 받으므로써 재생산이 가능함.\n",
    "- training and eval hooks: save 나 모니터링을 하는 곳에 쓰인다.\n",
    "- 그 외 트레인을 얼마나 할지 그 외의 것들을 받음\n",
    "\n",
    "그 이후, train과 eval을 learn_runner.run으로 받는다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "experiments = tf.contrib.learn.Experiment(\n",
    "        estimator = estimator,\n",
    "        train_input_fn = train_input_fn, #1st class fun\n",
    "        eval_input_fn = eval_input_fn,\n",
    "        train_steps = params.train_steps, #minibatch steps\n",
    "        min_eval_frequency=params.min_eval_frequency, #eval freq\n",
    "        train_monitors=[train_input, hook], #hooks for training\n",
    "        eval_steps=None #use eval. feeder until its empty\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learn_runner.run(\n",
    "    experiment_fn=experiment_fn,  # First-class function\n",
    "    run_config=run_config,  # RunConfig\n",
    "    schedule=\"train_and_evaluate\",  # What to run\n",
    "    hparams=params  # HParams\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "MNIST를 Dataset클레스와 Iterator를 사용하여 데이터를 학습시키고 할 것임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the training inputs\n",
    "def get_train_inputs(batch_size, mnist_data):\n",
    "    \"\"\"Return the input function to get the training data.\n",
    "    Args:\n",
    "        batch_size (int): Batch size of training iterator that is returned\n",
    "                          by the input function.\n",
    "        mnist_data (Object): Object holding the loaded mnist data.\n",
    "    Returns:\n",
    "        (Input function, IteratorInitializerHook):\n",
    "            - Function that returns (features, labels) when called.\n",
    "            - Hook to initialise input iterator.\n",
    "    \"\"\"\n",
    "    iterator_initializer_hook = IteratorInitializerHook()\n",
    "\n",
    "    def train_inputs():\n",
    "        \"\"\"Returns training set as Operations.\n",
    "        Returns:\n",
    "            (features, labels) Operations that iterate over the dataset\n",
    "            on every evaluation\n",
    "        \"\"\"\n",
    "        with tf.name_scope('Training_data'):\n",
    "            # Get Mnist data\n",
    "            images = mnist_data.train.images.reshape([-1, 28, 28, 1])\n",
    "            labels = mnist_data.train.labels\n",
    "            # Define placeholders\n",
    "            images_placeholder = tf.placeholder(\n",
    "                images.dtype, images.shape)\n",
    "            labels_placeholder = tf.placeholder(\n",
    "                labels.dtype, labels.shape)\n",
    "            # Build dataset iterator\n",
    "            dataset = tf.contrib.data.Dataset.from_tensor_slices(\n",
    "                (images_placeholder, labels_placeholder))\n",
    "            dataset = dataset.repeat(None)  # Infinite iterations\n",
    "            dataset = dataset.shuffle(buffer_size=10000)\n",
    "            dataset = dataset.batch(batch_size)\n",
    "            iterator = dataset.make_initializable_iterator()\n",
    "            next_example, next_label = iterator.get_next()\n",
    "            # Set runhook to initialize iterator\n",
    "            iterator_initializer_hook.iterator_initializer_func = \\\n",
    "                lambda sess: sess.run(\n",
    "                    iterator.initializer,\n",
    "                    feed_dict={images_placeholder: images,\n",
    "                               labels_placeholder: labels})\n",
    "            # Return batched (features, labels)\n",
    "            return next_example, next_label\n",
    "\n",
    "    # Return function and hook\n",
    "    return train_inputs, iterator_initializer_hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class IteratorInitializerHook(tf.train.SessionRunHook):\n",
    "    \"\"\"Hook to initialise data iterator after Session is created.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(IteratorInitializerHook, self).__init__()\n",
    "        self.iterator_initializer_func = None\n",
    "\n",
    "    def after_create_session(self, session, coord):\n",
    "        \"\"\"Initialise the iterator after the session has been created.\"\"\"\n",
    "        self.iterator_initializer_func(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:RunConfig.uid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_task_id': 0, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x120dc0f98>, '_master': '', '_num_ps_replicas': 0, '_num_worker_replicas': 0, '_environment': 'local', '_is_chief': True, '_evaluation_master': '', '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_secs': None, '_log_step_count_steps': 100, '_session_config': None, '_save_checkpoints_steps': 100, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_model_dir': './mnist_training'}\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting ./mnist_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting ./mnist_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting ./mnist_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting ./mnist_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:RunConfig.uid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "WARNING:tensorflow:From /anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/monitors.py:269: BaseMonitor.__init__ (from tensorflow.contrib.learn.python.learn.monitors) is deprecated and will be removed after 2016-12-05.\n",
      "Instructions for updating:\n",
      "Monitors are deprecated. Please use tf.train.SessionRunHook.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into ./mnist_training/model.ckpt.\n",
      "INFO:tensorflow:loss = 2.31533, step = 1\n",
      "INFO:tensorflow:Starting evaluation at 2017-09-21-15:18:23\n",
      "INFO:tensorflow:Restoring parameters from ./mnist_training/model.ckpt-1\n",
      "INFO:tensorflow:Finished evaluation at 2017-09-21-15:18:25\n",
      "INFO:tensorflow:Saving dict for global step 1: Accuracy = 0.098, global_step = 1, loss = 2.27705\n",
      "INFO:tensorflow:Validation (step 100): Accuracy = 0.098, loss = 2.27705, global_step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 101 into ./mnist_training/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 9.19167\n",
      "INFO:tensorflow:loss = 0.189803, step = 101 (10.881 sec)\n",
      "INFO:tensorflow:Starting evaluation at 2017-09-21-15:18:34\n",
      "INFO:tensorflow:Restoring parameters from ./mnist_training/model.ckpt-101\n",
      "INFO:tensorflow:Finished evaluation at 2017-09-21-15:18:36\n",
      "INFO:tensorflow:Saving dict for global step 101: Accuracy = 0.9629, global_step = 101, loss = 0.113973\n",
      "INFO:tensorflow:Validation (step 200): Accuracy = 0.9629, loss = 0.113973, global_step = 101\n",
      "INFO:tensorflow:Saving checkpoints for 201 into ./mnist_training/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 9.56029\n",
      "INFO:tensorflow:loss = 0.101367, step = 201 (10.459 sec)\n",
      "INFO:tensorflow:Starting evaluation at 2017-09-21-15:18:44\n",
      "INFO:tensorflow:Restoring parameters from ./mnist_training/model.ckpt-201\n",
      "INFO:tensorflow:Finished evaluation at 2017-09-21-15:18:47\n",
      "INFO:tensorflow:Saving dict for global step 201: Accuracy = 0.978, global_step = 201, loss = 0.0672728\n",
      "INFO:tensorflow:Validation (step 300): Accuracy = 0.978, loss = 0.0672728, global_step = 201\n",
      "INFO:tensorflow:Saving checkpoints for 301 into ./mnist_training/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 9.81877\n",
      "INFO:tensorflow:loss = 0.0458627, step = 301 (10.185 sec)\n",
      "INFO:tensorflow:Starting evaluation at 2017-09-21-15:18:55\n",
      "INFO:tensorflow:Restoring parameters from ./mnist_training/model.ckpt-301\n",
      "INFO:tensorflow:Finished evaluation at 2017-09-21-15:18:57\n",
      "INFO:tensorflow:Saving dict for global step 301: Accuracy = 0.9834, global_step = 301, loss = 0.0515568\n",
      "INFO:tensorflow:Validation (step 400): Accuracy = 0.9834, loss = 0.0515568, global_step = 301\n",
      "INFO:tensorflow:Saving checkpoints for 401 into ./mnist_training/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 10.0963\n",
      "INFO:tensorflow:loss = 0.102111, step = 401 (9.905 sec)\n",
      "INFO:tensorflow:Starting evaluation at 2017-09-21-15:19:05\n",
      "INFO:tensorflow:Restoring parameters from ./mnist_training/model.ckpt-401\n",
      "INFO:tensorflow:Finished evaluation at 2017-09-21-15:19:07\n",
      "INFO:tensorflow:Saving dict for global step 401: Accuracy = 0.9818, global_step = 401, loss = 0.0559934\n",
      "INFO:tensorflow:Validation (step 500): Accuracy = 0.9818, loss = 0.0559934, global_step = 401\n",
      "INFO:tensorflow:Saving checkpoints for 501 into ./mnist_training/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 9.66476\n",
      "INFO:tensorflow:loss = 0.0666304, step = 501 (10.347 sec)\n",
      "INFO:tensorflow:Starting evaluation at 2017-09-21-15:19:15\n",
      "INFO:tensorflow:Restoring parameters from ./mnist_training/model.ckpt-501\n",
      "INFO:tensorflow:Finished evaluation at 2017-09-21-15:19:17\n",
      "INFO:tensorflow:Saving dict for global step 501: Accuracy = 0.9857, global_step = 501, loss = 0.0422001\n",
      "INFO:tensorflow:Validation (step 600): Accuracy = 0.9857, loss = 0.0422001, global_step = 501\n",
      "INFO:tensorflow:Saving checkpoints for 601 into ./mnist_training/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 10.0999\n",
      "INFO:tensorflow:loss = 0.147653, step = 601 (9.901 sec)\n",
      "INFO:tensorflow:Starting evaluation at 2017-09-21-15:19:25\n",
      "INFO:tensorflow:Restoring parameters from ./mnist_training/model.ckpt-601\n",
      "INFO:tensorflow:Finished evaluation at 2017-09-21-15:19:27\n",
      "INFO:tensorflow:Saving dict for global step 601: Accuracy = 0.9856, global_step = 601, loss = 0.0445192\n",
      "INFO:tensorflow:Validation (step 700): Accuracy = 0.9856, loss = 0.0445192, global_step = 601\n",
      "INFO:tensorflow:Saving checkpoints for 701 into ./mnist_training/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 9.81656\n",
      "INFO:tensorflow:loss = 0.0560553, step = 701 (10.187 sec)\n",
      "INFO:tensorflow:Starting evaluation at 2017-09-21-15:19:35\n",
      "INFO:tensorflow:Restoring parameters from ./mnist_training/model.ckpt-701\n",
      "INFO:tensorflow:Finished evaluation at 2017-09-21-15:19:37\n",
      "INFO:tensorflow:Saving dict for global step 701: Accuracy = 0.9865, global_step = 701, loss = 0.0417034\n",
      "INFO:tensorflow:Validation (step 800): Accuracy = 0.9865, loss = 0.0417034, global_step = 701\n",
      "INFO:tensorflow:Saving checkpoints for 801 into ./mnist_training/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 10.154\n",
      "INFO:tensorflow:loss = 0.180448, step = 801 (9.849 sec)\n",
      "INFO:tensorflow:Starting evaluation at 2017-09-21-15:19:44\n",
      "INFO:tensorflow:Restoring parameters from ./mnist_training/model.ckpt-801\n",
      "INFO:tensorflow:Finished evaluation at 2017-09-21-15:19:47\n",
      "INFO:tensorflow:Saving dict for global step 801: Accuracy = 0.9846, global_step = 801, loss = 0.0453935\n",
      "INFO:tensorflow:Validation (step 900): Accuracy = 0.9846, loss = 0.0453935, global_step = 801\n",
      "INFO:tensorflow:Saving checkpoints for 901 into ./mnist_training/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 10.0593\n",
      "INFO:tensorflow:loss = 0.0439844, step = 901 (9.941 sec)\n",
      "INFO:tensorflow:Starting evaluation at 2017-09-21-15:19:55\n",
      "INFO:tensorflow:Restoring parameters from ./mnist_training/model.ckpt-901\n",
      "INFO:tensorflow:Finished evaluation at 2017-09-21-15:19:57\n",
      "INFO:tensorflow:Saving dict for global step 901: Accuracy = 0.9896, global_step = 901, loss = 0.0316949\n",
      "INFO:tensorflow:Validation (step 1000): Accuracy = 0.9896, loss = 0.0316949, global_step = 901\n",
      "INFO:tensorflow:Saving checkpoints for 1001 into ./mnist_training/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 9.72711\n",
      "INFO:tensorflow:loss = 0.0192028, step = 1001 (10.280 sec)\n",
      "INFO:tensorflow:Starting evaluation at 2017-09-21-15:20:06\n",
      "INFO:tensorflow:Restoring parameters from ./mnist_training/model.ckpt-1001\n",
      "INFO:tensorflow:Finished evaluation at 2017-09-21-15:20:08\n",
      "INFO:tensorflow:Saving dict for global step 1001: Accuracy = 0.9895, global_step = 1001, loss = 0.0328095\n",
      "INFO:tensorflow:Validation (step 1100): Accuracy = 0.9895, loss = 0.0328095, global_step = 1001\n",
      "INFO:tensorflow:Saving checkpoints for 1101 into ./mnist_training/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 9.41402\n",
      "INFO:tensorflow:loss = 0.0277784, step = 1101 (10.623 sec)\n",
      "INFO:tensorflow:Starting evaluation at 2017-09-21-15:20:16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./mnist_training/model.ckpt-1101\n",
      "INFO:tensorflow:Finished evaluation at 2017-09-21-15:20:18\n",
      "INFO:tensorflow:Saving dict for global step 1101: Accuracy = 0.9899, global_step = 1101, loss = 0.0338598\n",
      "INFO:tensorflow:Validation (step 1200): Accuracy = 0.9899, loss = 0.0338598, global_step = 1101\n",
      "INFO:tensorflow:Saving checkpoints for 1201 into ./mnist_training/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 9.42215\n",
      "INFO:tensorflow:loss = 0.0649293, step = 1201 (10.613 sec)\n",
      "INFO:tensorflow:Starting evaluation at 2017-09-21-15:20:27\n",
      "INFO:tensorflow:Restoring parameters from ./mnist_training/model.ckpt-1201\n",
      "INFO:tensorflow:Finished evaluation at 2017-09-21-15:20:29\n",
      "INFO:tensorflow:Saving dict for global step 1201: Accuracy = 0.9898, global_step = 1201, loss = 0.0362467\n",
      "INFO:tensorflow:Validation (step 1300): Accuracy = 0.9898, loss = 0.0362467, global_step = 1201\n",
      "INFO:tensorflow:Saving checkpoints for 1301 into ./mnist_training/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 9.39458\n",
      "INFO:tensorflow:loss = 0.00798693, step = 1301 (10.644 sec)\n",
      "INFO:tensorflow:Starting evaluation at 2017-09-21-15:20:37\n",
      "INFO:tensorflow:Restoring parameters from ./mnist_training/model.ckpt-1301\n",
      "INFO:tensorflow:Finished evaluation at 2017-09-21-15:20:39\n",
      "INFO:tensorflow:Saving dict for global step 1301: Accuracy = 0.9913, global_step = 1301, loss = 0.0275939\n",
      "INFO:tensorflow:Validation (step 1400): Accuracy = 0.9913, loss = 0.0275939, global_step = 1301\n",
      "INFO:tensorflow:Saving checkpoints for 1401 into ./mnist_training/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 10.011\n",
      "INFO:tensorflow:loss = 0.0324694, step = 1401 (9.989 sec)\n",
      "INFO:tensorflow:Starting evaluation at 2017-09-21-15:20:47\n",
      "INFO:tensorflow:Restoring parameters from ./mnist_training/model.ckpt-1401\n",
      "INFO:tensorflow:Finished evaluation at 2017-09-21-15:20:49\n",
      "INFO:tensorflow:Saving dict for global step 1401: Accuracy = 0.9908, global_step = 1401, loss = 0.0270458\n",
      "INFO:tensorflow:Validation (step 1500): Accuracy = 0.9908, loss = 0.0270458, global_step = 1401\n",
      "INFO:tensorflow:Saving checkpoints for 1501 into ./mnist_training/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 10.2504\n",
      "INFO:tensorflow:loss = 0.0794322, step = 1501 (9.755 sec)\n",
      "INFO:tensorflow:Starting evaluation at 2017-09-21-15:20:56\n",
      "INFO:tensorflow:Restoring parameters from ./mnist_training/model.ckpt-1501\n",
      "INFO:tensorflow:Finished evaluation at 2017-09-21-15:20:58\n",
      "INFO:tensorflow:Saving dict for global step 1501: Accuracy = 0.9912, global_step = 1501, loss = 0.0292888\n",
      "INFO:tensorflow:Validation (step 1600): Accuracy = 0.9912, loss = 0.0292888, global_step = 1501\n",
      "INFO:tensorflow:Saving checkpoints for 1601 into ./mnist_training/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 10.4153\n",
      "INFO:tensorflow:loss = 0.0544882, step = 1601 (9.602 sec)\n",
      "INFO:tensorflow:Starting evaluation at 2017-09-21-15:21:06\n",
      "INFO:tensorflow:Restoring parameters from ./mnist_training/model.ckpt-1601\n",
      "INFO:tensorflow:Finished evaluation at 2017-09-21-15:21:08\n",
      "INFO:tensorflow:Saving dict for global step 1601: Accuracy = 0.9908, global_step = 1601, loss = 0.0311543\n",
      "INFO:tensorflow:Validation (step 1700): Accuracy = 0.9908, loss = 0.0311543, global_step = 1601\n",
      "INFO:tensorflow:Saving checkpoints for 1701 into ./mnist_training/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 10.6042\n",
      "INFO:tensorflow:loss = 0.0339802, step = 1701 (9.430 sec)\n",
      "INFO:tensorflow:Starting evaluation at 2017-09-21-15:21:15\n",
      "INFO:tensorflow:Restoring parameters from ./mnist_training/model.ckpt-1701\n",
      "INFO:tensorflow:Finished evaluation at 2017-09-21-15:21:17\n",
      "INFO:tensorflow:Saving dict for global step 1701: Accuracy = 0.9878, global_step = 1701, loss = 0.0379412\n",
      "INFO:tensorflow:Validation (step 1800): Accuracy = 0.9878, loss = 0.0379412, global_step = 1701\n",
      "INFO:tensorflow:Saving checkpoints for 1801 into ./mnist_training/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 10.277\n",
      "INFO:tensorflow:loss = 0.0353006, step = 1801 (9.731 sec)\n",
      "INFO:tensorflow:Starting evaluation at 2017-09-21-15:21:25\n",
      "INFO:tensorflow:Restoring parameters from ./mnist_training/model.ckpt-1801\n",
      "INFO:tensorflow:Finished evaluation at 2017-09-21-15:21:27\n",
      "INFO:tensorflow:Saving dict for global step 1801: Accuracy = 0.9908, global_step = 1801, loss = 0.0296477\n",
      "INFO:tensorflow:Validation (step 1900): Accuracy = 0.9908, loss = 0.0296477, global_step = 1801\n",
      "INFO:tensorflow:Saving checkpoints for 1901 into ./mnist_training/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 10.2992\n",
      "INFO:tensorflow:loss = 0.0658273, step = 1901 (9.709 sec)\n",
      "INFO:tensorflow:Starting evaluation at 2017-09-21-15:21:35\n",
      "INFO:tensorflow:Restoring parameters from ./mnist_training/model.ckpt-1901\n",
      "INFO:tensorflow:Finished evaluation at 2017-09-21-15:21:37\n",
      "INFO:tensorflow:Saving dict for global step 1901: Accuracy = 0.9914, global_step = 1901, loss = 0.0280225\n",
      "INFO:tensorflow:Validation (step 2000): Accuracy = 0.9914, loss = 0.0280225, global_step = 1901\n",
      "INFO:tensorflow:Saving checkpoints for 2001 into ./mnist_training/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 9.93016\n",
      "INFO:tensorflow:loss = 0.0396168, step = 2001 (10.070 sec)\n",
      "INFO:tensorflow:Starting evaluation at 2017-09-21-15:21:45\n",
      "INFO:tensorflow:Restoring parameters from ./mnist_training/model.ckpt-2001\n",
      "INFO:tensorflow:Finished evaluation at 2017-09-21-15:21:47\n",
      "INFO:tensorflow:Saving dict for global step 2001: Accuracy = 0.9921, global_step = 2001, loss = 0.0275864\n",
      "INFO:tensorflow:Validation (step 2100): Accuracy = 0.9921, loss = 0.0275864, global_step = 2001\n",
      "INFO:tensorflow:Saving checkpoints for 2101 into ./mnist_training/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 9.96576\n",
      "INFO:tensorflow:loss = 0.0912239, step = 2101 (10.034 sec)\n",
      "INFO:tensorflow:Starting evaluation at 2017-09-21-15:21:55\n",
      "INFO:tensorflow:Restoring parameters from ./mnist_training/model.ckpt-2101\n",
      "INFO:tensorflow:Finished evaluation at 2017-09-21-15:21:57\n",
      "INFO:tensorflow:Saving dict for global step 2101: Accuracy = 0.9911, global_step = 2101, loss = 0.0322565\n",
      "INFO:tensorflow:Validation (step 2200): Accuracy = 0.9911, loss = 0.0322565, global_step = 2101\n",
      "INFO:tensorflow:Saving checkpoints for 2201 into ./mnist_training/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 9.94479\n",
      "INFO:tensorflow:loss = 0.00101517, step = 2201 (10.056 sec)\n",
      "INFO:tensorflow:Starting evaluation at 2017-09-21-15:22:05\n",
      "INFO:tensorflow:Restoring parameters from ./mnist_training/model.ckpt-2201\n",
      "INFO:tensorflow:Finished evaluation at 2017-09-21-15:22:07\n",
      "INFO:tensorflow:Saving dict for global step 2201: Accuracy = 0.9915, global_step = 2201, loss = 0.028396\n",
      "INFO:tensorflow:Validation (step 2300): Accuracy = 0.9915, loss = 0.028396, global_step = 2201\n",
      "INFO:tensorflow:Saving checkpoints for 2301 into ./mnist_training/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 10.2564\n",
      "INFO:tensorflow:loss = 0.018804, step = 2301 (9.750 sec)\n",
      "INFO:tensorflow:Starting evaluation at 2017-09-21-15:22:15\n",
      "INFO:tensorflow:Restoring parameters from ./mnist_training/model.ckpt-2301\n",
      "INFO:tensorflow:Finished evaluation at 2017-09-21-15:22:17\n",
      "INFO:tensorflow:Saving dict for global step 2301: Accuracy = 0.993, global_step = 2301, loss = 0.0261314\n",
      "INFO:tensorflow:Validation (step 2400): Accuracy = 0.993, loss = 0.0261314, global_step = 2301\n",
      "INFO:tensorflow:Saving checkpoints for 2401 into ./mnist_training/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 10.2765\n",
      "INFO:tensorflow:loss = 0.054462, step = 2401 (9.731 sec)\n",
      "INFO:tensorflow:Starting evaluation at 2017-09-21-15:22:25\n",
      "INFO:tensorflow:Restoring parameters from ./mnist_training/model.ckpt-2401\n",
      "INFO:tensorflow:Finished evaluation at 2017-09-21-15:22:27\n",
      "INFO:tensorflow:Saving dict for global step 2401: Accuracy = 0.9907, global_step = 2401, loss = 0.0309317\n",
      "INFO:tensorflow:Validation (step 2500): Accuracy = 0.9907, loss = 0.0309317, global_step = 2401\n",
      "INFO:tensorflow:Saving checkpoints for 2501 into ./mnist_training/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 9.74179\n",
      "INFO:tensorflow:loss = 0.011547, step = 2501 (10.265 sec)\n",
      "INFO:tensorflow:Starting evaluation at 2017-09-21-15:22:35\n",
      "INFO:tensorflow:Restoring parameters from ./mnist_training/model.ckpt-2501\n",
      "INFO:tensorflow:Finished evaluation at 2017-09-21-15:22:37\n",
      "INFO:tensorflow:Saving dict for global step 2501: Accuracy = 0.9931, global_step = 2501, loss = 0.0237308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Validation (step 2600): Accuracy = 0.9931, loss = 0.0237308, global_step = 2501\n",
      "INFO:tensorflow:Saving checkpoints for 2601 into ./mnist_training/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 9.90597\n",
      "INFO:tensorflow:loss = 0.00928678, step = 2601 (10.095 sec)\n",
      "INFO:tensorflow:Starting evaluation at 2017-09-21-15:22:45\n",
      "INFO:tensorflow:Restoring parameters from ./mnist_training/model.ckpt-2601\n",
      "INFO:tensorflow:Finished evaluation at 2017-09-21-15:22:47\n",
      "INFO:tensorflow:Saving dict for global step 2601: Accuracy = 0.9926, global_step = 2601, loss = 0.0298234\n",
      "INFO:tensorflow:Validation (step 2700): Accuracy = 0.9926, loss = 0.0298234, global_step = 2601\n",
      "INFO:tensorflow:Saving checkpoints for 2701 into ./mnist_training/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 10.1001\n",
      "INFO:tensorflow:loss = 0.0220058, step = 2701 (9.901 sec)\n",
      "INFO:tensorflow:Starting evaluation at 2017-09-21-15:22:55\n",
      "INFO:tensorflow:Restoring parameters from ./mnist_training/model.ckpt-2701\n",
      "INFO:tensorflow:Finished evaluation at 2017-09-21-15:22:57\n",
      "INFO:tensorflow:Saving dict for global step 2701: Accuracy = 0.9919, global_step = 2701, loss = 0.0292367\n",
      "INFO:tensorflow:Validation (step 2800): Accuracy = 0.9919, loss = 0.0292367, global_step = 2701\n",
      "INFO:tensorflow:Saving checkpoints for 2801 into ./mnist_training/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 10.1349\n",
      "INFO:tensorflow:loss = 0.0372308, step = 2801 (9.866 sec)\n",
      "INFO:tensorflow:Starting evaluation at 2017-09-21-15:23:05\n",
      "INFO:tensorflow:Restoring parameters from ./mnist_training/model.ckpt-2801\n",
      "INFO:tensorflow:Finished evaluation at 2017-09-21-15:23:07\n",
      "INFO:tensorflow:Saving dict for global step 2801: Accuracy = 0.9923, global_step = 2801, loss = 0.026002\n",
      "INFO:tensorflow:Validation (step 2900): Accuracy = 0.9923, loss = 0.026002, global_step = 2801\n",
      "INFO:tensorflow:Saving checkpoints for 2901 into ./mnist_training/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 9.89728\n",
      "INFO:tensorflow:loss = 0.0157941, step = 2901 (10.104 sec)\n",
      "INFO:tensorflow:Starting evaluation at 2017-09-21-15:23:15\n",
      "INFO:tensorflow:Restoring parameters from ./mnist_training/model.ckpt-2901\n",
      "INFO:tensorflow:Finished evaluation at 2017-09-21-15:23:17\n",
      "INFO:tensorflow:Saving dict for global step 2901: Accuracy = 0.9924, global_step = 2901, loss = 0.0239775\n",
      "INFO:tensorflow:Validation (step 3000): Accuracy = 0.9924, loss = 0.0239775, global_step = 2901\n",
      "INFO:tensorflow:Saving checkpoints for 3001 into ./mnist_training/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 10.0637\n",
      "INFO:tensorflow:loss = 0.0525903, step = 3001 (9.937 sec)\n",
      "INFO:tensorflow:Starting evaluation at 2017-09-21-15:23:25\n",
      "INFO:tensorflow:Restoring parameters from ./mnist_training/model.ckpt-3001\n",
      "INFO:tensorflow:Finished evaluation at 2017-09-21-15:23:27\n",
      "INFO:tensorflow:Saving dict for global step 3001: Accuracy = 0.9923, global_step = 3001, loss = 0.0285513\n",
      "INFO:tensorflow:Validation (step 3100): Accuracy = 0.9923, loss = 0.0285513, global_step = 3001\n",
      "INFO:tensorflow:Saving checkpoints for 3101 into ./mnist_training/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 9.86396\n",
      "INFO:tensorflow:loss = 0.00228201, step = 3101 (10.138 sec)\n",
      "INFO:tensorflow:Starting evaluation at 2017-09-21-15:23:35\n",
      "INFO:tensorflow:Restoring parameters from ./mnist_training/model.ckpt-3101\n",
      "INFO:tensorflow:Finished evaluation at 2017-09-21-15:23:37\n",
      "INFO:tensorflow:Saving dict for global step 3101: Accuracy = 0.9898, global_step = 3101, loss = 0.0399394\n",
      "INFO:tensorflow:Validation (step 3200): Accuracy = 0.9898, loss = 0.0399394, global_step = 3101\n",
      "INFO:tensorflow:Saving checkpoints for 3201 into ./mnist_training/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 10.1427\n",
      "INFO:tensorflow:loss = 0.00659313, step = 3201 (9.859 sec)\n",
      "INFO:tensorflow:Starting evaluation at 2017-09-21-15:23:45\n",
      "INFO:tensorflow:Restoring parameters from ./mnist_training/model.ckpt-3201\n",
      "INFO:tensorflow:Finished evaluation at 2017-09-21-15:23:47\n",
      "INFO:tensorflow:Saving dict for global step 3201: Accuracy = 0.9929, global_step = 3201, loss = 0.0247586\n",
      "INFO:tensorflow:Validation (step 3300): Accuracy = 0.9929, loss = 0.0247586, global_step = 3201\n",
      "INFO:tensorflow:Saving checkpoints for 3301 into ./mnist_training/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 10.4023\n",
      "INFO:tensorflow:loss = 0.0174388, step = 3301 (9.613 sec)\n",
      "INFO:tensorflow:Starting evaluation at 2017-09-21-15:23:55\n",
      "INFO:tensorflow:Restoring parameters from ./mnist_training/model.ckpt-3301\n",
      "INFO:tensorflow:Finished evaluation at 2017-09-21-15:23:57\n",
      "INFO:tensorflow:Saving dict for global step 3301: Accuracy = 0.9918, global_step = 3301, loss = 0.0296248\n",
      "INFO:tensorflow:Validation (step 3400): Accuracy = 0.9918, loss = 0.0296248, global_step = 3301\n",
      "INFO:tensorflow:Saving checkpoints for 3401 into ./mnist_training/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 9.85707\n",
      "INFO:tensorflow:loss = 0.00189566, step = 3401 (10.145 sec)\n",
      "INFO:tensorflow:Starting evaluation at 2017-09-21-15:24:05\n",
      "INFO:tensorflow:Restoring parameters from ./mnist_training/model.ckpt-3401\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-6befc9e36d57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m     tf.app.run(\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0mmain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_experiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m     )\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/platform/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m     46\u001b[0m   \u001b[0;31m# Call the main function, passing through any arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m   \u001b[0;31m# to the final program.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m   \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mflags_passthrough\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-6befc9e36d57>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(argv)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mrun_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_config\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# RunConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mschedule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train_and_evaluate\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# What to run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mhparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m  \u001b[0;31m# HParams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     )\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/learn_runner.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(experiment_fn, output_dir, schedule, run_config, hparams)\u001b[0m\n\u001b[1;32m    207\u001b[0m   \u001b[0mschedule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mschedule\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_get_default_schedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_execute_schedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschedule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/learn_runner.py\u001b[0m in \u001b[0;36m_execute_schedule\u001b[0;34m(experiment, schedule)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Allowed values for this experiment are: %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_tasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Schedule references non-callable member %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mschedule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/experiment.py\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    500\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_dir_suffix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eval_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m         )]\n\u001b[0;32m--> 502\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelay_secs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m     eval_result = self._call_evaluate(input_fn=self._eval_input_fn,\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/experiment.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, delay_secs)\u001b[0m\n\u001b[1;32m    278\u001b[0m     return self._call_train(input_fn=self._train_input_fn,\n\u001b[1;32m    279\u001b[0m                             \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m                             hooks=self._train_monitors + extra_hooks)\n\u001b[0m\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelay_secs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/experiment.py\u001b[0m in \u001b[0;36m_call_train\u001b[0;34m(self, _sentinel, input_fn, steps, hooks, max_steps)\u001b[0m\n\u001b[1;32m    670\u001b[0m                                    \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                                    \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m                                    hooks=hooks)\n\u001b[0m\u001b[1;32m    673\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m       return self._estimator.fit(input_fn=input_fn,\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps)\u001b[0m\n\u001b[1;32m    239\u001b[0m       \u001b[0mhooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStopAtStepHook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks)\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m           \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    516\u001b[0m                           \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m                           \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m                           run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    860\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                               \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m                               run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    863\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m         logging.info('An error was raised. This may be due to a preemption in '\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    978\u001b[0m               \u001b[0mresults\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m               \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 980\u001b[0;31m               run_metadata=run_metadata))\n\u001b[0m\u001b[1;32m    981\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_stop\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrun_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_requested\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/monitors.py\u001b[0m in \u001b[0;36mafter_run\u001b[0;34m(self, run_context, run_values)\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \"monitors\"] if \"monitors\" in run_values.results else {}\n\u001b[1;32m   1197\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_monitors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m       \u001b[0minduce_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1199\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0minduce_stop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m         \u001b[0mrun_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/monitors.py\u001b[0m in \u001b[0;36mstep_end\u001b[0;34m(self, step, output)\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEveryN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_every_n_step_begin_called\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevery_n_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/monitors.py\u001b[0m in \u001b[0;36mevery_n_step_end\u001b[0;34m(self, step, outputs)\u001b[0m\n\u001b[1;32m    694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m     \u001b[0;31m# Run evaluation and log it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m     \u001b[0mvalidation_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m     \u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalidation_outputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/monitors.py\u001b[0m in \u001b[0;36m_evaluate_estimator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    665\u001b[0m       return self._estimator.evaluate(\n\u001b[1;32m    666\u001b[0m           \u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m           name=self.name)\n\u001b[0m\u001b[1;32m    668\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m       return self._estimator.evaluate(\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, input_fn, steps, hooks, checkpoint_path, name)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m   def predict(self,\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_evaluate_model\u001b[0;34m(self, input_fn, hooks, checkpoint_path, name)\u001b[0m\n\u001b[1;32m    737\u001b[0m           \u001b[0mfinal_ops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m           \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m           config=self._session_config)\n\u001b[0m\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m       _write_dict_to_summary(\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/training/evaluation.py\u001b[0m in \u001b[0;36m_evaluate_once\u001b[0;34m(checkpoint_path, master, scaffold, eval_ops, feed_dict, final_ops, final_ops_feed_dict, hooks, config)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0meval_ops\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m   logging.info('Finished evaluation at ' + time.strftime('%Y-%m-%d-%H:%M:%S',\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    516\u001b[0m                           \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m                           \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m                           run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    860\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                               \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m                               run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    863\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m         logging.info('An error was raised. This may be due to a preemption in '\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    970\u001b[0m                                   \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m                                   \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m                                   run_metadata=run_metadata)\n\u001b[0m\u001b[1;32m    973\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"Script to illustrate usage of tf.estimator.Estimator in TF v1.3\"\"\"\n",
    "\n",
    "# Show debugging output\n",
    "tf.logging.set_verbosity(tf.logging.DEBUG)\n",
    "\n",
    "# Set default flags for the output directories\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "tf.app.flags.DEFINE_string(\n",
    "    flag_name='model_dir', default_value='./mnist_training',\n",
    "    docstring='Output directory for model and training stats.')\n",
    "tf.app.flags.DEFINE_string(\n",
    "    flag_name='data_dir', default_value='./mnist_data',\n",
    "    docstring='Directory to download the data to.')\n",
    "\n",
    "\n",
    "# Define and run experiment ###############################\n",
    "def run_experiment(argv=None):\n",
    "    \"\"\"Run the training experiment.\"\"\"\n",
    "    # Define model parameters\n",
    "    params = tf.contrib.training.HParams(\n",
    "        learning_rate=0.002,\n",
    "        n_classes=10,\n",
    "        train_steps=5000,\n",
    "        min_eval_frequency=100\n",
    "    )\n",
    "\n",
    "    # Set the run_config and the directory to save the model and stats\n",
    "    run_config = tf.contrib.learn.RunConfig()\n",
    "    run_config = run_config.replace(model_dir=FLAGS.model_dir)\n",
    "\n",
    "    learn_runner.run(\n",
    "        experiment_fn=experiment_fn,  # First-class function\n",
    "        run_config=run_config,  # RunConfig\n",
    "        schedule=\"train_and_evaluate\",  # What to run\n",
    "        hparams=params  # HParams\n",
    "    )\n",
    "\n",
    "\n",
    "def experiment_fn(run_config, params):\n",
    "    \"\"\"Create an experiment to train and evaluate the model.\n",
    "    Args:\n",
    "        run_config (RunConfig): Configuration for Estimator run.\n",
    "        params (HParam): Hyperparameters\n",
    "    Returns:\n",
    "        (Experiment) Experiment for training the mnist model.\n",
    "    \"\"\"\n",
    "    # You can change a subset of the run_config properties as\n",
    "    run_config = run_config.replace(\n",
    "        save_checkpoints_steps=params.min_eval_frequency)\n",
    "    # Define the mnist classifier\n",
    "    estimator = get_estimator(run_config, params)\n",
    "    # Setup data loaders\n",
    "    mnist = mnist_data.read_data_sets(FLAGS.data_dir, one_hot=False)\n",
    "    train_input_fn, train_input_hook = get_train_inputs(\n",
    "        batch_size=128, mnist_data=mnist)\n",
    "    eval_input_fn, eval_input_hook = get_test_inputs(\n",
    "        batch_size=128, mnist_data=mnist)\n",
    "    # Define the experiment\n",
    "    experiment = tf.contrib.learn.Experiment(\n",
    "        estimator=estimator,  # Estimator\n",
    "        train_input_fn=train_input_fn,  # First-class function\n",
    "        eval_input_fn=eval_input_fn,  # First-class function\n",
    "        train_steps=params.train_steps,  # Minibatch steps\n",
    "        min_eval_frequency=params.min_eval_frequency,  # Eval frequency\n",
    "        train_monitors=[train_input_hook],  # Hooks for training\n",
    "        eval_hooks=[eval_input_hook],  # Hooks for evaluation\n",
    "        eval_steps=None  # Use evaluation feeder until its empty\n",
    "    )\n",
    "    return experiment\n",
    "\n",
    "\n",
    "# Define model ############################################\n",
    "def get_estimator(run_config, params):\n",
    "    \"\"\"Return the model as a Tensorflow Estimator object.\n",
    "    Args:\n",
    "         run_config (RunConfig): Configuration for Estimator run.\n",
    "         params (HParams): hyperparameters.\n",
    "    \"\"\"\n",
    "    return tf.estimator.Estimator(\n",
    "        model_fn=model_fn,  # First-class function\n",
    "        params=params,  # HParams\n",
    "        config=run_config  # RunConfig\n",
    "    )\n",
    "\n",
    "\n",
    "def model_fn(features, labels, mode, params):\n",
    "    \"\"\"Model function used in the estimator.\n",
    "    Args:\n",
    "        features (Tensor): Input features to the model.\n",
    "        labels (Tensor): Labels tensor for training and evaluation.\n",
    "        mode (ModeKeys): Specifies if training, evaluation or prediction.\n",
    "        params (HParams): hyperparameters.\n",
    "    Returns:\n",
    "        (EstimatorSpec): Model to be run by Estimator.\n",
    "    \"\"\"\n",
    "    is_training = mode == ModeKeys.TRAIN\n",
    "    # Define model's architecture\n",
    "    logits = architecture(features, is_training=is_training)\n",
    "    predictions = tf.argmax(logits, axis=-1)\n",
    "    # Loss, training and eval operations are not needed during inference.\n",
    "    loss = None\n",
    "    train_op = None\n",
    "    eval_metric_ops = {}\n",
    "    if mode != ModeKeys.INFER:\n",
    "        loss = tf.losses.sparse_softmax_cross_entropy(\n",
    "            labels=tf.cast(labels, tf.int32),\n",
    "            logits=logits)\n",
    "        train_op = get_train_op_fn(loss, params)\n",
    "        eval_metric_ops = get_eval_metric_ops(labels, predictions)\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode,\n",
    "        predictions=predictions,\n",
    "        loss=loss,\n",
    "        train_op=train_op,\n",
    "        eval_metric_ops=eval_metric_ops\n",
    "    )\n",
    "\n",
    "\n",
    "def get_train_op_fn(loss, params):\n",
    "    \"\"\"Get the training Op.\n",
    "    Args:\n",
    "         loss (Tensor): Scalar Tensor that represents the loss function.\n",
    "         params (HParams): Hyperparameters (needs to have `learning_rate`)\n",
    "    Returns:\n",
    "        Training Op\n",
    "    \"\"\"\n",
    "    return tf.contrib.layers.optimize_loss(\n",
    "        loss=loss,\n",
    "        global_step=tf.contrib.framework.get_global_step(),\n",
    "        optimizer=tf.train.AdamOptimizer,\n",
    "        learning_rate=params.learning_rate\n",
    "    )\n",
    "\n",
    "\n",
    "def get_eval_metric_ops(labels, predictions):\n",
    "    \"\"\"Return a dict of the evaluation Ops.\n",
    "    Args:\n",
    "        labels (Tensor): Labels tensor for training and evaluation.\n",
    "        predictions (Tensor): Predictions Tensor.\n",
    "    Returns:\n",
    "        Dict of metric results keyed by name.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'Accuracy': tf.metrics.accuracy(\n",
    "            labels=labels,\n",
    "            predictions=predictions,\n",
    "            name='accuracy')\n",
    "    }\n",
    "\n",
    "\n",
    "def architecture(inputs, is_training, scope='MnistConvNet'):\n",
    "    \"\"\"Return the output operation following the network architecture.\n",
    "    Args:\n",
    "        inputs (Tensor): Input Tensor\n",
    "        is_training (bool): True iff in training mode\n",
    "        scope (str): Name of the scope of the architecture\n",
    "    Returns:\n",
    "         Logits output Op for the network.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope):\n",
    "        with slim.arg_scope(\n",
    "                [slim.conv2d, slim.fully_connected],\n",
    "                weights_initializer=tf.contrib.layers.xavier_initializer()):\n",
    "            net = slim.conv2d(inputs, 20, [5, 5], padding='VALID',\n",
    "                              scope='conv1')\n",
    "            net = slim.max_pool2d(net, 2, stride=2, scope='pool2')\n",
    "            net = slim.conv2d(net, 40, [5, 5], padding='VALID',\n",
    "                              scope='conv3')\n",
    "            net = slim.max_pool2d(net, 2, stride=2, scope='pool4')\n",
    "            net = tf.reshape(net, [-1, 4 * 4 * 40])\n",
    "            net = slim.fully_connected(net, 256, scope='fn5')\n",
    "            net = slim.dropout(net, is_training=is_training,\n",
    "                               scope='dropout5')\n",
    "            net = slim.fully_connected(net, 256, scope='fn6')\n",
    "            net = slim.dropout(net, is_training=is_training,\n",
    "                               scope='dropout6')\n",
    "            net = slim.fully_connected(net, 10, scope='output',\n",
    "                                       activation_fn=None)\n",
    "        return net\n",
    "\n",
    "\n",
    "# Define data loaders #####################################\n",
    "class IteratorInitializerHook(tf.train.SessionRunHook):\n",
    "    \"\"\"Hook to initialise data iterator after Session is created.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(IteratorInitializerHook, self).__init__()\n",
    "        self.iterator_initializer_func = None\n",
    "\n",
    "    def after_create_session(self, session, coord):\n",
    "        \"\"\"Initialise the iterator after the session has been created.\"\"\"\n",
    "        self.iterator_initializer_func(session)\n",
    "\n",
    "\n",
    "# Define the training inputs\n",
    "def get_train_inputs(batch_size, mnist_data):\n",
    "    \"\"\"Return the input function to get the training data.\n",
    "    Args:\n",
    "        batch_size (int): Batch size of training iterator that is returned\n",
    "                          by the input function.\n",
    "        mnist_data (Object): Object holding the loaded mnist data.\n",
    "    Returns:\n",
    "        (Input function, IteratorInitializerHook):\n",
    "            - Function that returns (features, labels) when called.\n",
    "            - Hook to initialise input iterator.\n",
    "    \"\"\"\n",
    "    iterator_initializer_hook = IteratorInitializerHook()\n",
    "\n",
    "    def train_inputs():\n",
    "        \"\"\"Returns training set as Operations.\n",
    "        Returns:\n",
    "            (features, labels) Operations that iterate over the dataset\n",
    "            on every evaluation\n",
    "        \"\"\"\n",
    "        with tf.name_scope('Training_data'):\n",
    "            # Get Mnist data\n",
    "            images = mnist_data.train.images.reshape([-1, 28, 28, 1])\n",
    "            labels = mnist_data.train.labels\n",
    "            # Define placeholders\n",
    "            images_placeholder = tf.placeholder(\n",
    "                images.dtype, images.shape)\n",
    "            labels_placeholder = tf.placeholder(\n",
    "                labels.dtype, labels.shape)\n",
    "            # Build dataset iterator\n",
    "            dataset = tf.contrib.data.Dataset.from_tensor_slices(\n",
    "                (images_placeholder, labels_placeholder))\n",
    "            dataset = dataset.repeat(None)  # Infinite iterations\n",
    "            dataset = dataset.shuffle(buffer_size=10000)\n",
    "            dataset = dataset.batch(batch_size)\n",
    "            iterator = dataset.make_initializable_iterator()\n",
    "            next_example, next_label = iterator.get_next()\n",
    "            # Set runhook to initialize iterator\n",
    "            iterator_initializer_hook.iterator_initializer_func = \\\n",
    "                lambda sess: sess.run(\n",
    "                    iterator.initializer,\n",
    "                    feed_dict={images_placeholder: images,\n",
    "                               labels_placeholder: labels})\n",
    "            # Return batched (features, labels)\n",
    "            return next_example, next_label\n",
    "\n",
    "    # Return function and hook\n",
    "    return train_inputs, iterator_initializer_hook\n",
    "\n",
    "\n",
    "def get_test_inputs(batch_size, mnist_data):\n",
    "    \"\"\"Return the input function to get the test data.\n",
    "    Args:\n",
    "        batch_size (int): Batch size of training iterator that is returned\n",
    "                          by the input function.\n",
    "        mnist_data (Object): Object holding the loaded mnist data.\n",
    "    Returns:\n",
    "        (Input function, IteratorInitializerHook):\n",
    "            - Function that returns (features, labels) when called.\n",
    "            - Hook to initialise input iterator.\n",
    "    \"\"\"\n",
    "    iterator_initializer_hook = IteratorInitializerHook()\n",
    "\n",
    "    def test_inputs():\n",
    "        \"\"\"Returns training set as Operations.\n",
    "        Returns:\n",
    "            (features, labels) Operations that iterate over the dataset\n",
    "            on every evaluation\n",
    "        \"\"\"\n",
    "        with tf.name_scope('Test_data'):\n",
    "            # Get Mnist data\n",
    "            images = mnist_data.test.images.reshape([-1, 28, 28, 1])\n",
    "            labels = mnist_data.test.labels\n",
    "            # Define placeholders\n",
    "            images_placeholder = tf.placeholder(\n",
    "                images.dtype, images.shape)\n",
    "            labels_placeholder = tf.placeholder(\n",
    "                labels.dtype, labels.shape)\n",
    "            # Build dataset iterator\n",
    "            dataset = tf.contrib.data.Dataset.from_tensor_slices(\n",
    "                (images_placeholder, labels_placeholder))\n",
    "            dataset = dataset.batch(batch_size)\n",
    "            iterator = dataset.make_initializable_iterator()\n",
    "            next_example, next_label = iterator.get_next()\n",
    "            # Set runhook to initialize iterator\n",
    "            iterator_initializer_hook.iterator_initializer_func = \\\n",
    "                lambda sess: sess.run(\n",
    "                    iterator.initializer,\n",
    "                    feed_dict={images_placeholder: images,\n",
    "                               labels_placeholder: labels})\n",
    "            return next_example, next_label\n",
    "\n",
    "    # Return function and hook\n",
    "    return test_inputs, iterator_initializer_hook\n",
    "\n",
    "\n",
    "# Run script ##############################################\n",
    "if __name__ == \"__main__\":\n",
    "    tf.app.run(\n",
    "        main=run_experiment\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Script to illustrate inference of a trained tf.estimator.Estimator.\n",
    "NOTE: This is dependent on mnist_estimator.py which defines the model.\n",
    "mnist_estimator.py can be found at:\n",
    "https://gist.github.com/peterroelants/9956ec93a07ca4e9ba5bc415b014bcca\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import skimage.io\n",
    "import tensorflow as tf\n",
    "\n",
    "from mnist_estimator import get_estimator\n",
    "\n",
    "\n",
    "# Set default flags for the output directories\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "tf.app.flags.DEFINE_string(\n",
    "    flag_name='saved_model_dir', default_value='./mnist_training',\n",
    "    docstring='Output directory for model and training stats.')\n",
    "\n",
    "\n",
    "# MNIST sample images\n",
    "IMAGE_URLS = [\n",
    "    'https://i.imgur.com/SdYYBDt.png',  # 0\n",
    "    'https://i.imgur.com/Wy7mad6.png',  # 1\n",
    "    'https://i.imgur.com/nhBZndj.png',  # 2\n",
    "    'https://i.imgur.com/V6XeoWZ.png',  # 3\n",
    "    'https://i.imgur.com/EdxBM1B.png',  # 4\n",
    "    'https://i.imgur.com/zWSDIuV.png',  # 5\n",
    "    'https://i.imgur.com/Y28rZho.png',  # 6\n",
    "    'https://i.imgur.com/6qsCz2W.png',  # 7\n",
    "    'https://i.imgur.com/BVorzCP.png',  # 8\n",
    "    'https://i.imgur.com/vt5Edjb.png',  # 9\n",
    "]\n",
    "\n",
    "\n",
    "def infer(argv=None):\n",
    "    \"\"\"Run the inference and print the results to stdout.\"\"\"\n",
    "    params = tf.contrib.training.HParams()  # Empty hyperparameters\n",
    "    # Set the run_config where to load the model from\n",
    "    run_config = tf.contrib.learn.RunConfig()\n",
    "    run_config = run_config.replace(model_dir=FLAGS.saved_model_dir)\n",
    "    # Initialize the estimator and run the prediction\n",
    "    estimator = get_estimator(run_config, params)\n",
    "    result = estimator.predict(input_fn=test_inputs)\n",
    "    for r in result:\n",
    "        print(r)\n",
    "\n",
    "\n",
    "def test_inputs():\n",
    "    \"\"\"Returns training set as Operations.\n",
    "    Returns:\n",
    "        (features, ) Operations that iterate over the test set.\n",
    "    \"\"\"\n",
    "    with tf.name_scope('Test_data'):\n",
    "        images = tf.constant(load_images(), dtype=np.float32)\n",
    "        dataset = tf.contrib.data.Dataset.from_tensor_slices((images,))\n",
    "        # Return as iteration in batches of 1\n",
    "        return dataset.batch(1).make_one_shot_iterator().get_next()\n",
    "\n",
    "\n",
    "def load_images():\n",
    "    \"\"\"Load MNIST sample images from the web and return them in an array.\n",
    "    Returns:\n",
    "        Numpy array of size (10, 28, 28, 1) with MNIST sample images.\n",
    "    \"\"\"\n",
    "    images = np.zeros((10, 28, 28, 1))\n",
    "    for idx, url in enumerate(IMAGE_URLS):\n",
    "        images[idx, :, :, 0] = skimage.io.imread(url)\n",
    "    return images\n",
    "\n",
    "\n",
    "# Run script ##############################################\n",
    "if __name__ == \"__main__\":\n",
    "    tf.app.run(main=infer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
