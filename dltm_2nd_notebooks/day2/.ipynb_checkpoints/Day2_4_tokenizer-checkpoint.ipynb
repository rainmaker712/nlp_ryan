{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Max Score Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "score = {'파스': 0.3, '파스타': 0.7, '좋아요': 0.2, '좋아':0.5}\n",
    "sent = '파스타가좋아요'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "가능한 모든 subwords를 추출하고, 해당 점수를 부여합니다. \n",
    "\n",
    "형식은 (word, begin, end, score) 입니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('파스', 0, 2, 0.3),\n",
       " ('파스타', 0, 3, 0.7),\n",
       " ('스타', 1, 3, 0),\n",
       " ('스타가', 1, 4, 0),\n",
       " ('타가', 2, 4, 0),\n",
       " ('타가좋', 2, 5, 0),\n",
       " ('가좋', 3, 5, 0),\n",
       " ('가좋아', 3, 6, 0),\n",
       " ('좋아', 4, 6, 0.5),\n",
       " ('좋아요', 4, 7, 0.2),\n",
       " ('아요', 5, 7, 0)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subtokens = []\n",
    "\n",
    "for b in range(0, len(sent)):\n",
    "    for r in range(2, 3+1):\n",
    "        \n",
    "        e = b + r\n",
    "        \n",
    "        if e > len(sent):\n",
    "            continue\n",
    "            \n",
    "        subtoken = sent[b:e]\n",
    "        \n",
    "        # (subtoken, 시작점, 끝점, 단어 점수)\n",
    "        subtokens.append((subtoken, b, e, score.get(subtoken, 0)))\n",
    "        \n",
    "subtokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "점수를 기준으로 정렬합니다. '파스타', 좋아'가 맨 위로 올라옴을 볼 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('파스타', 0, 3, 0.7),\n",
      " ('좋아', 4, 6, 0.5),\n",
      " ('파스', 0, 2, 0.3),\n",
      " ('좋아요', 4, 7, 0.2),\n",
      " ('스타', 1, 3, 0),\n",
      " ('스타가', 1, 4, 0),\n",
      " ('타가', 2, 4, 0),\n",
      " ('타가좋', 2, 5, 0),\n",
      " ('가좋', 3, 5, 0),\n",
      " ('가좋아', 3, 6, 0),\n",
      " ('아요', 5, 7, 0)]\n"
     ]
    }
   ],
   "source": [
    "subtokens = sorted(subtokens, key=lambda x:x[3], reverse=True)\n",
    "pprint(subtokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## begin, end가 겹치는 부분 지우기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "list.pop(0)은 0번째 item을 return하고, 해당 item을 list에서 지우는 함수입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subtoken = 파스타\n",
      "\n",
      "[('좋아', 4, 6, 0.5),\n",
      " ('파스', 0, 2, 0.3),\n",
      " ('좋아요', 4, 7, 0.2),\n",
      " ('스타', 1, 3, 0),\n",
      " ('스타가', 1, 4, 0),\n",
      " ('타가', 2, 4, 0),\n",
      " ('타가좋', 2, 5, 0),\n",
      " ('가좋', 3, 5, 0),\n",
      " ('가좋아', 3, 6, 0),\n",
      " ('아요', 5, 7, 0)]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "word, b, e, s = subtokens.pop(0)\n",
    "\n",
    "print('subtoken = %s\\n' % word)\n",
    "pprint(subtokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파스타와 겹치는 부분\n",
      "('파스', 0, 2, 0.3)\n",
      "('스타', 1, 3, 0)\n",
      "('스타가', 1, 4, 0)\n",
      "('타가', 2, 4, 0)\n",
      "('타가좋', 2, 5, 0)\n",
      "\n",
      "중복된 subtokens을 지운 뒤\n",
      "[('좋아', 4, 6, 0.5),\n",
      " ('좋아요', 4, 7, 0.2),\n",
      " ('가좋', 3, 5, 0),\n",
      " ('가좋아', 3, 6, 0),\n",
      " ('아요', 5, 7, 0)]\n"
     ]
    }
   ],
   "source": [
    "results.append((word, b, e, s))\n",
    "\n",
    "removals = []\n",
    "for i, (word_, b_, e_, _) in enumerate(subtokens):\n",
    "    \n",
    "    # word와 오버랩 되는 word_\n",
    "    if (b_ < e and b < e_):\n",
    "        removals.append(i)\n",
    "\n",
    "print('파스타와 겹치는 부분')\n",
    "for i in removals:\n",
    "    print(subtokens[i])\n",
    "\n",
    "print('\\n중복된 subtokens을 지운 뒤')\n",
    "for i in reversed(removals):\n",
    "    del subtokens[i]\n",
    "    \n",
    "pprint(subtokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "subtokens가 빌 때 까지 이 작업을 계속하면 MaxScoreTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('파스타', 0, 3, 0.7, 3),\n",
       " ('가', 3, 4, 0, 1),\n",
       " ('좋아', 4, 6, 0.5, 2),\n",
       " ('요', 6, 7, 0, 1)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(sent, score, max_len=3):\n",
    "    \n",
    "    def initialize(sent, score, max_len=3):\n",
    "        subtokens = []\n",
    "        \n",
    "        for b in range(0, len(sent)):\n",
    "            for r in range(2, max_len+1):\n",
    "                \n",
    "                e = b + r\n",
    "                if e > len(sent):\n",
    "                    continue\n",
    "                    \n",
    "                subtoken = sent[b:e]\n",
    "                subtokens.append((subtoken, b, e, score.get(subtoken, 0), e - b))\n",
    "                \n",
    "        if not subtokens:\n",
    "            return subtokens\n",
    "        \n",
    "        # Sort by (score and its length)\n",
    "        subtokens = sorted(subtokens, key=lambda x:x[3], reverse=True)\n",
    "#         subtokens = sorted(subtokens, key=lambda x:(x[3], x[4]), reverse=True)\n",
    "        return subtokens\n",
    "    \n",
    "    def _tokenize(subtokens):\n",
    "        results = []\n",
    "        \n",
    "        while subtokens:\n",
    "            \n",
    "            word, b, e, s, l = subtokens.pop(0)\n",
    "            results.append((word, b, e, s, l))\n",
    "            \n",
    "            # Select overlapped subtoken\n",
    "            removals = []\n",
    "            for i, (word_, b_, e_, _1, _2) in enumerate(subtokens):\n",
    "                if (b_ < e and b < e_):\n",
    "                    removals.append(i)\n",
    "                    \n",
    "            # Remove them\n",
    "            for i in reversed(removals):\n",
    "                del subtokens[i]\n",
    "                \n",
    "        # Sort by begin point\n",
    "        results = sorted(results, key=lambda x:x[1])\n",
    "        return results\n",
    "    \n",
    "    def postprocess(sent, results):\n",
    "        # 맨 앞글자가 비었을 경우, \n",
    "        if results[0][1] != 0:\n",
    "            b = 0\n",
    "            e = results[0][1]\n",
    "            word = sent[b:e]\n",
    "            results.insert(0, (word, b, e, 0, e - b))\n",
    "            \n",
    "        # 맨 뒷글자가 비었을 경우\n",
    "        if results[-1][2] != len(sent):\n",
    "            b = results[-1][2]\n",
    "            e = len(sent)\n",
    "            word = sent[b:e]\n",
    "            results.append((word, b, e, 0, e - b))\n",
    "        \n",
    "        # 중간 글자가 비었을 경우\n",
    "        adds = []\n",
    "        for i, base in enumerate(results[:-1]):\n",
    "            if base[2] == results[i+1][1]:\n",
    "                continue            \n",
    "            b = base[2]\n",
    "            e = results[i+1][1]\n",
    "            word = sent[b:e]\n",
    "            adds.append((word, b, e, 0, e - b))\n",
    "        \n",
    "        results = sorted(results + adds, key=lambda x:x[1])\n",
    "        return results\n",
    "            \n",
    "    subtokens = initialize(sent, score, max_len)\n",
    "    if not subtokens:\n",
    "        return [(sent, 0, len(sent), 0)]\n",
    "    \n",
    "    results = _tokenize(subtokens)\n",
    "    results = postprocess(sent, results)\n",
    "\n",
    "    return results\n",
    "\n",
    "tokenize(sent, score)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('난', 0, 1, 0, 1),\n",
       " ('파스타', 1, 4, 0.7, 3),\n",
       " ('가', 4, 5, 0, 1),\n",
       " ('좋아', 5, 7, 0.5, 2),\n",
       " ('요', 7, 8, 0, 1)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize('난파스타가좋아요', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
